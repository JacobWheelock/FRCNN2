{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f32a31-cc2f-4ded-ad13-0ea0ae0d17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    To handle the data loading as different images may have different number \n",
    "    of objects and to handle varying size tensors as well.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        A.RandomRotate90(0.5),\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc',\n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "# define the validation transforms\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc', \n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "\n",
    "def show_tranformed_image(train_loader):\n",
    "    \"\"\"\n",
    "    This function shows the transformed images from the `train_loader`.\n",
    "    Helps to check whether the tranformed images along with the corresponding\n",
    "    labels are correct or not.\n",
    "    Only runs if `VISUALIZE_TRANSFORMED_IMAGES = True`\n",
    "    \"\"\"\n",
    "    if len(train_loader) > 0:\n",
    "        for i in range(1):\n",
    "            images, targets = next(iter(train_loader))\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "            sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            for box in boxes:\n",
    "                cv2.rectangle(sample,\n",
    "                            (box[0], box[1]),\n",
    "                            (box[2], box[3]),\n",
    "                            (0, 0, 255), 2)\n",
    "            cv2.imshow('Transformed image', sample)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "class getDataset(Dataset):\n",
    "    def __init__(self, dir_path, width, height, classes, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.dir_path = dir_path\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.classes = classes\n",
    "        \n",
    "        \n",
    "        image_extensions = ['jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'webp']\n",
    "        all_extensions = image_extensions + [ext.upper() for ext in image_extensions]  # Add uppercase versions\n",
    "        self.image_paths = glob.glob(f\"{self.dir_path}/*.png\")\n",
    "        for extension in all_extensions:\n",
    "            self.image_paths.extend(glob.glob(f\"{self.dir_path}/*.{extension}\"))\n",
    "        # get all the image paths in sorted order\n",
    "        \n",
    "        self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n",
    "        self.all_images = sorted(self.all_images)\n",
    "    def __getitem__(self, idx):\n",
    "        # capture the image name and the full image path\n",
    "        image_name = self.all_images[idx]\n",
    "        image_path = self.dir_path + '/' + image_name\n",
    "        # read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        # convert BGR to RGB color format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "        \n",
    "        # capture the corresponding XML file for getting the annotations\n",
    "        annot_filename = image_name[:-4] + '.xml'\n",
    "        annot_file_path = self.dir_path + '/' + annot_filename\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        print(idx)\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # get the height and width of the image\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "\n",
    "        \n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "        for member in root.findall('object'):\n",
    "            # map the current object name to `classes` list to get...\n",
    "            # ... the label index and append to `labels` list\n",
    "            try:\n",
    "                labels.append(self.classes.index(member.find('class').text))\n",
    "            except:\n",
    "                labels.append(self.classes.index(member.find('label').text))\n",
    "            try:\n",
    "                # xmin = left corner x-coordinates\n",
    "                xmin = int(member.find('xmin').text)\n",
    "            except:\n",
    "                # xmin = left corner x-coordinates\n",
    "                xmin = int(member.find('x').text)    \n",
    "            try:\n",
    "                # xmax = right corner x-coordinates\n",
    "                xmax = int(member.find('xmax').text)\n",
    "            except:\n",
    "                # xmax = right corner x-coordinates\n",
    "                xmax = xmin + int(member.find('width').text)  \n",
    "            try:\n",
    "                # ymin = left corner y-coordinates\n",
    "                ymin = int(member.find('ymin').text)\n",
    "            except:\n",
    "                # xmin = left corner y-coordinates\n",
    "                ymin = int(member.find('y').text)   \n",
    "            try:\n",
    "                # ymax = right corner x-coordinates\n",
    "                ymax = int(member.find('ymax').text)\n",
    "            except:\n",
    "                # xmin = left corner y-coordinates\n",
    "                ymax = ymin + int(member.find('height').text)   \n",
    "            \n",
    "            # resize the bounding boxes according to the...\n",
    "            # ... desired `width`, `height`\n",
    "            xmin_final = (xmin/image_width)*self.width\n",
    "            xmax_final = (xmax/image_width)*self.width\n",
    "            ymin_final = (ymin/image_height)*self.height\n",
    "            ymax_final = (ymax/image_height)*self.height\n",
    "            \n",
    "            boxes.append([xmin_final, ymin_final, xmax_final, ymax_final])\n",
    "        \n",
    "        # bounding box to tensor\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # no crowd instances\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        # labels to tensor\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "        # apply the image transforms\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = image_resized,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return image_resized, target\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "def get_loaders(train_dataset, valid_dataset, BATCH_SIZE, collate_fn):\n",
    "    train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "    return [train_loader, valid_loader]\n",
    "\n",
    "# function to visualize a single sample\n",
    "def visualize_sample(image, target):\n",
    "    for i in range(0,len(target['boxes'])):\n",
    "        box = target['boxes'][i]\n",
    "    \n",
    "        label = CLASSES[target['labels'][i]]\n",
    "    \n",
    "        cv2.rectangle(\n",
    "            image, \n",
    "            (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "            (0, 255, 0), 1\n",
    "            )\n",
    "        cv2.putText(\n",
    "            image, label, (int(box[0]), int(box[1]-5)), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
    "            )\n",
    "        plt.imshow((image*255).astype('uint8'))\n",
    "\n",
    "def visualize_samples(NUM_SAMPLES, TRAIN_DIR, RESIZE_TO, CLASSES):\n",
    "    dataset = getDataset(\n",
    "        TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES\n",
    "    )\n",
    "    for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "        image, target = dataset[i]\n",
    "        plt.figure()\n",
    "        visualize_sample(image, target)\n",
    "\n",
    "def create_model(num_classes):\n",
    "    \n",
    "    # load Faster RCNN pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='COCO_V1')\n",
    "    \n",
    "    # get the number of input features \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "    return model\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def saveBoxesClassesScores(boxesFileName, classFileName, scoreFileName, boxes, classes, scores, OUT_DIR):\n",
    "    classPath = OUT_DIR + '/' + classFileName + '.csv'\n",
    "    boxPath = OUT_DIR + '/' + boxFileName + '.csv'\n",
    "    scorePath = OUT_DIR + '/' + scoreFileName + '.csv'\n",
    "    with open(boxPath, 'w', newline='') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        for el in boxes:\n",
    "            if (type(el) == type(None)):\n",
    "                writer.writerow([0])\n",
    "            else:\n",
    "                writer.writerow(el)\n",
    "    with open(classPath, 'w', newline='') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        for el in classes:\n",
    "            if (type(el) == type(None)):\n",
    "                writer.writerow([0])\n",
    "            else:\n",
    "                writer.writerow(el)\n",
    "\n",
    "    with open(scorePath, 'w', newline='') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        for el in scores:\n",
    "            if (type(el) == type(None)):\n",
    "                writer.writerow([0])\n",
    "            else:\n",
    "                writer.writerow(el)\n",
    "\n",
    "def load_model(model_name, MODEL_DIR, NUM_CLASSES):\n",
    "    # set the computation device\n",
    "    modelPath = './models/' + model_name\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # load the model and the trained weights\n",
    "    model = create_model(num_classes=NUM_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(\n",
    "        modelPath, map_location=device\n",
    "    ))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model_train(model_name, MODEL_DIR, NUM_CLASSES):\n",
    "    # set the computation device\n",
    "    modelPath = './models/' + model_name\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # load the model and the trained weights\n",
    "    model = create_model(num_classes=NUM_CLASSES).to(device)\n",
    "    model.load_state_dict(torch.load(\n",
    "        modelPath, map_location=device\n",
    "    ))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
