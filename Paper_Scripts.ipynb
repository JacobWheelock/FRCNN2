{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes: first entry is reserved for background, DEFINE CLASSES HERE\n",
    "CLASSES = [\n",
    "    'frog'\n",
    "]\n",
    "NUM_CLASSES = len(CLASSES) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fffe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 # increase / decrease according to GPU memory\n",
    "RESIZE_TO = 512 # resize the image for training and transforms\n",
    "NUM_EPOCHS = 5 # number of epochs to train for\n",
    "SAVE_PLOTS_EPOCH = 1 # save loss plots after these many epochs\n",
    "SAVE_MODEL_EPOCH = 5 # save model after these many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa757ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the final datasets and data loaders\n",
    "train_dataset = getDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform())\n",
    "valid_dataset = getDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\n",
    "[train_loader, valid_loader] = get_loaders(train_dataset, valid_dataset, BATCH_SIZE, collate_fn)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "exp_results = run_experiment(train_dataset, valid_dataset, NUM_CLASSES, BATCH_SIZE, \n",
    "                             NUM_EXPERIMENTS=2, EPOCHS_PER_EXPERIMENT=5, TRIALS_PER_EXPERIMENT=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47703938",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.3# 0.9 by default\n",
    "model_name = 'experiment_model100.pth'\n",
    "model = load_model(model_name, MODEL_DIR, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_images_figs(DIR_TEST, model, OUT_DIR, detection_threshold, CLASSES):\n",
    "    imagePath = glob.glob(f\"{DIR_TEST}/*.png\")\n",
    "    image_extensions = ['jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'webp']\n",
    "    all_extensions = image_extensions + [ext.upper() for ext in image_extensions]  # Add uppercase versions\n",
    "    for extension in all_extensions:\n",
    "        imagePath.extend(glob.glob(f\"{DIR_TEST}/*.{extension}\"))\n",
    "\n",
    "    all_images = [image_path.split('/')[-1] for image_path in imagePath]\n",
    "    all_images = sorted(all_images)\n",
    "    num_images = len(all_images)\n",
    "    classes = [None] * num_images\n",
    "    bboxes = [None] * num_images\n",
    "    sscores = [None] * num_images\n",
    "    \n",
    "    for idx, el in enumerate(all_images):\n",
    "        orig_image = cv2.imread(DIR_TEST + '/' + el)\n",
    "        # BGR to RGB\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        # Normalize the pixel values (between 0 and 1)\n",
    "        image /= 255.0\n",
    "        # Rearrange color channels\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(float)\n",
    "        # Convert to tensor\n",
    "        image_tensor = torch.tensor(image, dtype=torch.float).cuda() if torch.cuda.is_available() else torch.tensor(image, dtype=torch.float)\n",
    "        # Add batch dimension\n",
    "        image_tensor = torch.unsqueeze(image_tensor, 0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)\n",
    "        \n",
    "        outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "        \n",
    "        if len(outputs[0]['boxes']) != 0:\n",
    "            boxes = outputs[0]['boxes'].data.numpy()\n",
    "            scores = outputs[0]['scores'].data.numpy()\n",
    "            sscores[idx] = scores[scores >= detection_threshold]\n",
    "            boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "            bboxes[idx] = boxes\n",
    "            draw_boxes = boxes.copy() \n",
    "            \n",
    "            pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "            pred_classes = np.array(pred_classes)\n",
    "            pred_classes = pred_classes[scores >= detection_threshold]\n",
    "            classes[idx] = pred_classes\n",
    "            \n",
    "            for j, box in enumerate(draw_boxes):\n",
    "                x1, y1, x2, y2 = box\n",
    "                cv2.rectangle(orig_image, (x1, y1), (x2, y2), (0, 0, 255), 10)\n",
    "                cv2.putText(orig_image, str(pred_classes[j]), (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                # Extract and enlarge the detected region\n",
    "                detected_img = orig_image[y1:y2, x1:x2]\n",
    "                factor = 8  # Change factor to desired zoom\n",
    "                enlarged_img = cv2.resize(detected_img, None, fx=factor, fy=factor, interpolation=cv2.INTER_LINEAR)\n",
    "                \n",
    "                # Calculate where to place the enlarged image on the original\n",
    "                eh, ew, _ = enlarged_img.shape\n",
    "                ex, ey = 3000, 900  # Starting coordinates for the enlarged image (top left)\n",
    "                if (j == 1):\n",
    "                    ex, ey = 3000,50\n",
    "                \n",
    "                # Ensure the enlarged image does not go out of the bounds of the original image\n",
    "                if ey + eh > orig_image.shape[0]:\n",
    "                    ey = orig_image.shape[0] - eh\n",
    "                if ex + ew > orig_image.shape[1]:\n",
    "                    ex = orig_image.shape[1] - ew\n",
    "                \n",
    "                # Overlay the enlarged image on the original image\n",
    "                orig_image[ey:ey+eh, ex:ex+ew] = enlarged_img\n",
    "                \n",
    "                # Draw lines connecting the small and enlarged boxes\n",
    "                cv2.line(orig_image, (x2, y1), (ex, ey), (255, 0, 0), 10)\n",
    "                cv2.line(orig_image, (x2, y2), (ex, ey + eh), (255, 0, 0), 10)\n",
    "\n",
    "            cv2.imwrite(OUT_DIR + '/' + el, orig_image)  # Save the modified image\n",
    "\n",
    "        print(f\"Image {idx+1} done...\")\n",
    "        print('-'*50)\n",
    "\n",
    "    print('TEST PREDICTIONS COMPLETE') \n",
    "    return [bboxes, classes, sscores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TEST = './test_data/test_images/'\n",
    "[bboxes, classes, sscores] = inference_images_figs(DIR_TEST, model, OUT_DIR, detection_threshold, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b031c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
